##第一步
# 导入基本包
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')   #警告过滤器，跳过警告
##第二步，数据处理
# 导入数据
weather = pd.read_csv('E:\研究生阶段文件\软件学习\数据集\降雨数据\天气数据.csv') 
#用均值来补充缺失值
weather.iloc[:,3:-1]=weather.iloc[:,3:-1].fillna(weather.iloc[:,3:-1].median())
#将所有的列标签变为小写
weather.rename(str.lower,axis='columns',inplace=True)
#创建一个label对象
from sklearn.preprocessing import LabelEncoder
label=LabelEncoder()
#调用Label对象内的fit_transform来将raintomorroww转化成数值型（都转换成int64）
weather["raintomorrow"]=label.fit_transform(weather["raintomorrow"])
weather.info()
weather.head()
##第三步划分测试集和训练集
x=weather[["humidity9am","humidity3pm","pressure9am","pressure3pm","cloud9am","cloud3pm"]]
y=weather[["raintomorrow"]]
x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.65, random_state=0)
##第四步利用贝叶斯的optuna确定最优参数
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.65, random_state=1412)
###第四步贝叶斯优化
##贝叶斯的优化器导入（主要有三种这边采用的是hyperopt )
import sklearn.svm as svm
import hyperopt
from hyperopt import hp, fmin, tpe, Trials, partial
from hyperopt.early_stop import no_progress_loss
from sklearn.model_selection import cross_val_score
#定义目标函数
def hyperopt_objective(params):
#设置SVM优化器
    reg = svm.SVC(kernel =params["kernel"]
                  ,C =params["C"]
                  ,gamma =params["gamma"]
                  ,degree=params["degree"]
                  ,random_state=1412
                  ,verbose=False
            )
#Hyperopt只支持寻找f(x)的最小值，不支持寻找最大值，因此当我们定义的目标函数是某种正面的评估指标时（如准确率）
#我们需要对该评估指标取负。如果我们定义的目标函数是负损失，也需要对负损失取绝对值。当且仅当我们定义的目标函数是普通损失时
#我们才不需要改变输出

    score =-1.0 * cross_val_score(reg, x_train, y_train)
    return score.mean()
##定义参数空p.choice返回最优参数的索引，0就是指linear
param_grid_simple = {'kernel': hp.choice('kernel', ['linear', 'rbf',"poly"]),
                     'C': hp.uniform('C', 0, 10.0),
                     'gamma': hp.uniform('gamma', 0, 20.0),
                     'degree': hp.choice('degree', [*range(0,5,1)]),
                   
                    }
#具体化优化流程
def param_hyperopt(max_evals=100):   
    # 记录迭代过程
    trials=Trials()
    early_stop_fn=no_progress_loss(100) # 当损失函数的连续迭代100次都没有下降时，则停止
    params_best=fmin(hyperopt_objective 
#fmin作用就是在有限的迭代次数下，求使目标函数fn取得最小值的超参数(超参数的阈值在space中)，在space中搜索超参数的方法使由algo进行指定
                     ,space=param_grid_simple
                     ,trials=trials#Trials对象用于保存所有超参数、损失和其他信息，也就是迭代的结果展示在最后
                     ,max_evals=max_evals # 设定迭代次数
                     ,algo=tpe.suggest #algo：超参数搜索算法,tpe.suggest为超参数空间的顺序搜索提供逻辑
                   
                    )


    
    print('best parmas:',params_best)
    return params_best,trials
from sklearn.model_selection import cross_val_score
import sklearn.svm as svm
import optuna
#定义函数
def optuna_objective(trial):   
#定义参数空间
    kernel=trial.suggest_categorical("kernel",["linear","poly","rbf"]) 
    C = trial.suggest_float("C",1,10) 
    gamma = trial.suggest_float("gamma",0,20)
    degree = trial.suggest_int("degree",0,5)
#定义评估器
    reg = svm.SVC(kernel =kernel 
                  ,C = C
                  ,gamma = gamma
                  ,degree =degree
                  ,random_state=1412
                  ,verbose=False
                 
                 )
    score = cross_val_score(reg, x_train, y_train)
    return score.mean()
##具体化优化流程
def optimizer_optuna(n_trials, algo):
    
#定义使用TPE或者GP
    if algo == "TPE":
        #树结构 Parzen 算法
        algo = optuna.samplers.TPESampler(n_startup_trials = 10, n_ei_candidates = 24)#用于计算预期改进的候选样本数，
    elif algo == "GP":
        from optuna.integration import SkoptSampler
        import skopt
        algo = SkoptSampler(skopt_kwargs={'base_estimator':'GP', #选择高斯过程
                                          'n_initial_points':10, #初始观测点10个
                                          'acq_func':'EI'} #选择的采集函数为EI，期望增量
                           )
    
#实际优化过程，首先实例化优化器
    study = optuna.create_study(sampler = algo #要使用的具体算法
                                , direction="minimize" #优化的方向，可以填写minimize或maximize
                               )
#开始优化
    study.optimize(optuna_objective #目标函数+参数范围
                   , n_trials=n_trials #迭代次数
                   , show_progress_bar=True #展示进度条
                  )
#打印最佳参数与最佳分数
    print("\n","\n","best params: ", study.best_trial.params,
          "\n","\n","best score: ", study.best_trial.values,
          "\n")
    
    return study.best_trial.params, study.best_trial.values
best_params, best_score = optimizer_optuna(10,"GP") #我们这边采用GP进行迭代
params_best = param_hyperopt(10)
##第五步利用最优参数进行建立SVM
clf1=svm.SVC(kernel='rbf',gamma=0.001,C=10)
#计算准确率
from sklearn.metrics import accuracy_score
clf1.fit(x_train, y_train)
y_pred = clf1.predict(x_test)
print("SVM准确率:", accuracy_score(y_test, y_pred))
##特征重要性分析
import eli5
from eli5.sklearn import PermutationImportance
perm = PermutationImportance(clf1,random_state=0).fit(x_test, y_test)
eli5.show_weights(perm, feature_names = x_test.columns.tolist())